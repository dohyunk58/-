{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dependency ì„¤ì¹˜(ë§¤ pullë§ˆë‹¤ ì‹¤í–‰í•˜ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama indexë¥¼ ì‚¬ìš©í•œ gemini ì±—ë´‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .env íŒŒì¼ì„ í†µí•´ API key ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "#load .env\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### png ë¶ˆëŸ¬ì˜¤ê¸°(tesseractë¥¼ ì‚¬ìš©í•œ OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from llama_index.core import Document\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# ë¬¸ì„œì˜ í˜•ì‹ = í…ìŠ¤íŠ¸ \n",
    "# tesseract ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë”°ë¡œ ì„¤ì¹˜ë¥¼ í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤\n",
    "# íŠœí† ë¦¬ì–¼ : https://www.allmyuniverse.com/implementing-python-ocr-with-tesseract/ \n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "image_path = r'data_png\\dwld.png'\n",
    "doc2_text = pytesseract.image_to_string(Image.open(image_path), lang='kor')\n",
    "\n",
    "# ê°ì²´ë¡œ ë³€í™˜í•˜ê¸° \n",
    "doc2 = Document(text=doc2_text)\n",
    "# Document ê°ì²´ ì¶œë ¥\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdf í•™ìŠµ ë°ì´í„° ìœ„ì¹˜ ì„¤ì • í›„ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ìœ„ì¹˜ ì„¤ì • í›„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "input_dir = r\"data_pdf\"\n",
    "reader = SimpleDirectoryReader(input_dir=input_dir)\n",
    "doc1 = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdf OCRë¡œ í…ìŠ¤íŠ¸ ë³€í™˜(ì˜ˆì •)\n",
    "https://medium.com/@dr.booma19/extracting-text-from-pdf-files-using-ocr-a-step-by-step-guide-with-python-code-becf221529ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒì¼ ê²°í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = doc1 + [doc2] \n",
    "# print(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì„ë² ë”© ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model_ko = HuggingFaceEmbedding(model_name=\"bespin-global/klue-sroberta-base-continue-learning-by-mnr\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llama index ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "llm = Gemini(model_name='models/gemini-1.5-flash', request_timeout=120.0)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=800, chunk_overlap=20, embed_model=embed_model_ko)\n",
    "index = VectorStoreIndex.from_documents(all_docs,service_context=service_context,show_progress=True)\n",
    "\n",
    "index.storage_context.persist()\n",
    "\n",
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë©€í‹°í„´ ì§ˆë¬¸ê³¼ ë‹µë³€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©€í‹° í„´ ëŒ€í™”ë¥¼ ìœ„í•œ history ë¦¬ìŠ¤íŠ¸\n",
    "history = []\n",
    "\n",
    "def ask_query(query, history):\n",
    "    # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— í˜„ì¬ ì¿¼ë¦¬ë¥¼ ì¶”ê°€\n",
    "    history.append({\"role\": \"user\", \"content\": query})\n",
    "    \n",
    "    # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³‘í•©\n",
    "    formatted_history = \"\\n\".join([f\"{item['role']}: {item['content']}\" for item in history])\n",
    "    \n",
    "    # ì¿¼ë¦¬ ì—”ì§„ì— í˜„ì¬ íˆìŠ¤í† ë¦¬ë¥¼ ì „ë‹¬í•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = query_engine.query(formatted_history)\n",
    "     \n",
    "    # ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
    "    history.append({\"role\": \"ai\", \"content\": response})\n",
    "    \n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜ˆì‹œ ëŒ€í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query1 = \"í•´ìš´ë²•ì— ëŒ€í•´ ì„¤ëª…\"\n",
    "# response1 = ask_query(query1, history)\n",
    "# print(f\"Model: {response1}\")\n",
    "\n",
    "# query2 = \"ë” ìì„¸íˆ ì•Œë ¤ì¤˜\"\n",
    "# response2 = ask_query(query2, history)\n",
    "# print(f\"Model: {response2}\")\n",
    "\n",
    "# query3 = \"ì¹¸ì˜ˆ ì›¨ìŠ¤íŠ¸ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\"\n",
    "# response3 = ask_query(query3, history)\n",
    "# print(f\"Model: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flaskë¡œ ì›¹ ì„œë²„ êµ¬ë™í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [02/Sep/2024 00:47:12] \"POST /chatbot HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: ì €ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê³  ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì…ë‹ˆë‹¤. ğŸ˜Š \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Sep/2024 00:47:21] \"POST /chatbot HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: ì €ëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ì •ë³´ë¥¼ í•™ìŠµí–ˆì§€ë§Œ, íŠ¹íˆ í•´ìš´ë²•ì— ëŒ€í•œ ì •ë³´ë¥¼ ë§ì´ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ğŸ˜Š \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/chatbot', methods=['POST'])\n",
    "def chatbot_response():\n",
    "    user_input = request.json.get('message')\n",
    "    # ì—¬ê¸°ì— ì±—ë´‡ ë¡œì§ì„ ì¶”ê°€í•˜ì„¸ìš”.\n",
    "    response = ask_query(user_input, history)\n",
    "    return jsonify({'response': str(response)})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
